import torch
import torch.nn as nn
from torchvision import datasets, transforms
from torch.utils.data import DataLoader
from torch.autograd import Variable
import time


# 模型选择，cnn or fnn
model_select = 'cnn'
# 是否使用gpu，未写完
gpu_train = False


# 定义前馈神经网络模型
class FNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.Sequential(
            nn.Linear(2352, 10),
            nn.ReLU(),
            nn.Linear(10, 10),
            nn.ReLU(),
            nn.Linear(10, 10)
        )

    def forward(self, x):
        x = self.layers(x)
        return x

# 定义卷积神经网络模型
class myCNN(nn.Module):
    def __init__(self):
        super().__init__()
        # 设置卷积层
        self.conv = nn.Sequential()
        self.conv.add_module('conv1', nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1))
        self.conv.add_module('activ1', nn.ReLU())
        self.conv.add_module('conv2', nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1))
        self.conv.add_module('activ2', nn.ReLU())
        self.conv.add_module('max1', nn.MaxPool2d(stride=2, kernel_size=2))
        # 设置全连接层
        self.dense = nn.Sequential()
        self.dense.add_module('dense1', nn.Linear(14*14*128, 1024))
        self.dense.add_module('activ3', nn.ReLU())
        self.dense.add_module('dropout', nn.Dropout(p=0.5))
        self.dense.add_module('dense2', nn.Linear(1024, 10))
        # self.dense.add_module('activ4', nn.Sigmoid())

    def forward(self, x):
        x = self.conv(x)
        x = x.view(-1, 14*14*128)
        x = self.dense(x)
        return x


if __name__ == '__main__':
    # 查看gpu参数并打印
    if torch.cuda.is_available():
        gpu_train = True
        gpu_num = torch.cuda.device_count()
    print(gpu_train, gpu_num)
    # model = FNN()
    # print(model)
    # 设置图片转换（预处理）参数
    transform = transforms.Compose([transforms.ToTensor(),
                                    transforms.Lambda(lambda x: x.repeat(3, 1, 1)),
                                    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])
    # 使用dataset抽象MINIST
    data_train = datasets.MNIST(root='./data', transform=transform, train=True, download=True)
    data_test = datasets.MNIST(root='./data', transform=transform, train=False)
    # """
    # 查看batch里面的内容
    # """
    # data_output = datasets.MNIST(root='./data', train=True, download=True)
    # data_output_loader = DataLoader(dataset=data_output,
    #                                 batch_size=64,
    #                                 shuffle=True)
    # for data in data_output_loader:
    #     print(data[0].size())

    
    # 使用Dataloader处理MINIST
    data_loader_train = DataLoader(dataset=data_train,
                                   batch_size=64,
                                   shuffle=True)
    data_loader_test = DataLoader(dataset=data_test,
                                 batch_size=64,
                                 shuffle=True)
    
    # Dataloader为可迭代对象，每一个对象为一个mini_batch,大小在使用Dataloader读取datasets时设置
    # 确切说为batch数目
    print('训练数据条目：' + str(len(data_loader_train)) + '\n测试数据条目：' + str(len(data_loader_test)))
    # for inputs, label in data_loader_train:
        # print(inputs.size())
        # print(label.size())
        # for each in inputs:
        #     print(each)
        #     break
        # break
    
    # 设置是选择卷积神经网络模型还是前馈神经网络模型
    if model_select == 'cnn':
        net_model = myCNN()
    elif model_select == 'fnn':
        net_model = FNN()
    # 设置代价函数为交叉熵损失
    cost = torch.nn.CrossEntropyLoss()
    # 设置优化器，梯度下价法优化模型参数，在此设置学习率为0.05
    optimzer = torch.optim.SGD(net_model.parameters(), lr=0.05)
    ep_index = 0
    # 记录训练开始时间
    t_start = time.time()
    # 设置迭代次数epoch为0.5
    for epoch in range(5):
        ep_index = ep_index + 1
        index = 0
        # 迭代dataloader处理的数据集对象，每一次迭代一个epoch送入模型
        for data in data_loader_train:
            # print(data)
            # break
            X_train, y_train = data
            # print(y_train)
            # print(type(X_train))
            # print(X_train.size())
            X_train, y_train = Variable(X_train), Variable(y_train)
            # X_train维度为[64, 3, 28, 28], batch_size, channels, pic_size
            # print(X_train.size())
            # print(type(X_train))
            if model_select == 'fnn':
                output = net_model(X_train.view(-1, 3 * 28 * 28))
            elif model_select == 'cnn':
                output = net_model(X_train)
            _, prediction = torch.max(output, 1)
            train_correct = (prediction == y_train).sum().item()
            # 前向传播计算损失
            loss = cost(output, y_train)
            # 优化器梯度清零
            optimzer.zero_grad()
            # 误差反向传播
            loss.backward()
            # 使用优化器更新模型参数
            optimzer.step()
            index = index + 1
            t_spend = time.time() - t_start
            print('epoch:' + str(ep_index) + ', train loss:' + str(loss) + ', accu:' + str(train_correct/64 * 100) + '%, train num:' + str(index) + ', time_spend:' + str(t_spend))
        test_ind = 0
        for data in data_loader_test:
            test_ind = test_ind + 1
            X_test, y_test = data
            X_test, y_test = Variable(X_test), Variable(y_test)
            if model_select == 'fnn':
                output = net_model(X_test.view(-1, 3 * 28 * 28))
            elif model_select == 'cnn':
                output = net_model(X_test)
            _, prediction = torch.max(output, 1)
            test_correct = (prediction == y_test).sum().item()
            loss = cost(output, y_test)
            t_spend = time.time() - t_start
            print('epoch:' + str(ep_index) + ', test loss:' + str(loss) + ', accu:' + str(test_correct/64 * 100) + '%, test num:' + str(test_ind) + ', time_spend:' + str(t_spend))
    t_end = time.time() - t_start
    print('trained time：' + str(t_end))

